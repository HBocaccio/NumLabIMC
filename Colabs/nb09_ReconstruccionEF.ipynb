{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HBocaccio/NumLabIMC/blob/main/Colabs/nb09_ReconstruccionEF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> (Última Actualización: 8 de Octubre de 2025)"
      ],
      "metadata": {
        "id": "nG9dXKXv5wMc"
      },
      "id": "nG9dXKXv5wMc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivación de este colab\n",
        "<!-- $$\\require{amsmath}$$ -->\n",
        "\n",
        "Recordemos que estamos estudiando **sistemas dinámicos**, con reglas representadas por **ecuaciones diferenciales ordinarias (ODEs)**. Ya vimos como obtener las soluciones numéricamente teniendo las ODEs, y cómo reconstruir las ODEs cuando tengo datos que son una representación subsampleada de las variables. Particularmente, vimos cómo podemos extraer estos datos a partir de videos, con un poco de implementación manual y de ingeniería de atributos.\n",
        "\n",
        "En esta clase vamos a ver cómo extraer de manera automática una representación topológicamente equivalente al comportamiento dinámico de un sistema. Vamos a **recontruir el espacio de fases** usando métodos lineales de extracción de modos empíricos como SVD, y usando la representación en el espacio latente de redes neuronales con arquitectura de tipo **autoencoders**."
      ],
      "metadata": {
        "id": "QY8cmGm_-YOl"
      },
      "id": "QY8cmGm_-YOl"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yu76ETnBZj8Y"
      },
      "id": "yu76ETnBZj8Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cdef76bc",
      "metadata": {
        "id": "cdef76bc"
      },
      "source": [
        "# Modos Empíricos con SVD\n",
        "\n",
        "En este Notebook vamos a trabajar con datos experimentales correspondientes a una película del aparato fonador de un ave. En este sistema una membrana oscila debido a la interacción con un flujo de aire (mismo fenómeno que sucede, por ejemplo, en nuestras cuerdas vocales).\n",
        "\n",
        "Aquí la película en cuestión:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9702228b",
      "metadata": {
        "id": "9702228b"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(url='http://materias.df.uba.ar/sdeiaaamdda2020c2/files/2012/07/membranagif.gif')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los datos"
      ],
      "metadata": {
        "id": "dXyQ231QZfG4"
      },
      "id": "dXyQ231QZfG4"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Cargar GIF desde la URL\n",
        "url = 'http://materias.df.uba.ar/sdeiaaamdda2020c2/files/2012/07/membranagif.gif'\n",
        "response = requests.get(url)\n",
        "gif = imageio.mimread(BytesIO(response.content))\n",
        "\n",
        "# Convertir a array (n_frames, height, width, 3)\n",
        "x = np.array(gif)\n",
        "print(\"Forma original:\", x.shape)\n",
        "\n",
        "# Convertir a escala de grises si está en RGB\n",
        "if x.shape[-1] == 3:\n",
        "    X = np.mean(x, axis=-1)  # promedio de los canales RGB\n",
        "else:\n",
        "    X = x.squeeze()  # por si ya viene en un solo canal\n",
        "\n",
        "# Normalizar a [0,1]\n",
        "X = X.astype('float32') / 255.0\n",
        "\n",
        "# Restar la media\n",
        "X_medio = np.mean(X)\n",
        "X = X - X_medio\n",
        "\n",
        "print(\"Forma final de X:\", X.shape)\n",
        "print(\"Valor medio (debería ser ~0):\", np.mean(X))\n"
      ],
      "metadata": {
        "id": "zhABr7w0ZecB"
      },
      "id": "zhABr7w0ZecB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0d701976",
      "metadata": {
        "id": "0d701976"
      },
      "source": [
        "Como verán, los archivos constan de 95 frames (imágenes) de la película."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3eaaf8e",
      "metadata": {
        "id": "d3eaaf8e"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Graficamos un frame como ejemplo\n",
        "numero = 55\n",
        "plt.matshow(X[numero],cmap='gray')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1f85991",
      "metadata": {
        "id": "c1f85991"
      },
      "source": [
        "Vamos a \"aplastar\" el tensor que contiene las 95 imágenes. Es decir que vamos a llevarlo a una forma $(n,m)$ siendo $n = altura \\times base$ y siendo $ m = \\# \\  frames$ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d31eb0",
      "metadata": {
        "id": "24d31eb0"
      },
      "outputs": [],
      "source": [
        "# Tomamos las dimensiones de X\n",
        "dims = np.shape(X)\n",
        "\n",
        "# Construyo una matriz Y de dimensión n x m con n = altura x base, m = num de frames\n",
        "Y = np.transpose(np.reshape(X,(dims[0],dims[1]*dims[2])))\n",
        "\n",
        "print('Shape de la matrix original',np.shape(X))\n",
        "print('Shape de la matrix aplanada',np.shape(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "594b2488",
      "metadata": {
        "id": "594b2488"
      },
      "source": [
        "**Aplicamos SVD**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58477b8a",
      "metadata": {
        "id": "58477b8a"
      },
      "source": [
        "Usaremos la función de la librería de álgebra lineal de numpy `np.linalg.svd` para realizar la descomposición."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1cac3b",
      "metadata": {
        "id": "3c1cac3b"
      },
      "outputs": [],
      "source": [
        "# SVD para los primeros N frames\n",
        "N = 95\n",
        "\n",
        "# # SVD Completo\n",
        "# Uhat, Shat, Vhat = np.linalg.svd(Y[:,:N], full_matrices=True)\n",
        "\n",
        "# SVD Económico\n",
        "Uhat, Shat, Vhat = np.linalg.svd(Y[:,:N], full_matrices=False)\n",
        "\n",
        "print('Shape de U:', Uhat.shape)\n",
        "print('Shape de Shat:', Shat.shape)\n",
        "print('Shape de Vhat:', Vhat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2bc9762",
      "metadata": {
        "id": "c2bc9762"
      },
      "source": [
        "**Ejercicio:**\n",
        "Cambien del SVD completo al SVD económico y vean cómo cambia la forma de las matrices. ¿Qué es lo que está cambiando entre ambos manera de computar la descomposición?\n",
        "\n",
        "Vamos a graficar el valor de cada uno de los valores singulares y la suma acumulada de los mismos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61207e3f",
      "metadata": {
        "id": "61207e3f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(Shat,'o')\n",
        "plt.ylabel('Singular value')\n",
        "plt.grid()\n",
        "plt.yscale('linear')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.cumsum(Shat)/np.sum(Shat),'o')\n",
        "plt.ylabel('Suma cumulativa')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5aa4c76",
      "metadata": {
        "id": "a5aa4c76"
      },
      "source": [
        "Noten que en la matriz Uhat guardamos la información de los modos espaciales como columnas, mientras que en la matriz Vhat guardamos la evolución temporal de cada uno de estos modos como filas.\n",
        "\n",
        "Veamos como lucen los primeros 6 modos espaciales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703b8079",
      "metadata": {
        "id": "703b8079"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    plt.imshow(np.reshape(Uhat[:,i],(dims[1],dims[2])),cmap='gray',vmin=np.min(Uhat[:,i]),vmax=np.max(Uhat[:,i]))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7351430e",
      "metadata": {
        "id": "7351430e"
      },
      "source": [
        "**Evolución Temporal de los modos**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1ba60e",
      "metadata": {
        "id": "8b1ba60e"
      },
      "source": [
        "Exploremos cómo evolucionan temporalmente los primeros 4 modos empíricos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459194af",
      "metadata": {
        "id": "459194af"
      },
      "outputs": [],
      "source": [
        "# PRIMEROS MODOS TEMPORALES\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(Vhat[0],'.-',label='V0')\n",
        "plt.plot(Vhat[1],'.-',label='V1')\n",
        "plt.plot(Vhat[2],'.-',label='V2')\n",
        "plt.plot(Vhat[3],'.-',label='V3')\n",
        "plt.title('Evolución temporal de los primeros 4 modos')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "modo = 1\n",
        "plt.plot(Vhat[modo],'.-',label='Modo elegido')\n",
        "plt.title('Evolución temporal de un modo')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9ec8712",
      "metadata": {
        "id": "c9ec8712"
      },
      "source": [
        "Si graficamos la evolución temporal de los primeros 3 modos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531991db",
      "metadata": {
        "id": "531991db"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
        "ax.plot(Vhat[0],Vhat[1],Vhat[2])\n",
        "ax.set_xlabel('PC0')\n",
        "ax.set_ylabel('PC1')\n",
        "ax.set_zlabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a23cdf6",
      "metadata": {
        "id": "9a23cdf6"
      },
      "source": [
        "**Discusión:** Observando la imagen de los modos espaciales y su evolución temporal, responda:\n",
        "\n",
        "*   ¿Podríamos pensar que la evolución de los primeros 3 modos se da dentro de un espacio de fases? ¿Qué tipo de solución sería?\n",
        "*   Observe el primer modo espacial ¿Parece estar involucrado en la dinámica de la membrana que oscila? ¿A qué podría corresponder?\n",
        "*   Si tuviese que elegir dos modos que representen la dinámica de oscilación, ¿cuáles eligiríá? Haga un gráfico con únicamente esos modos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8bdcce",
      "metadata": {
        "id": "0b8bdcce"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5,5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(Vhat[1],Vhat[2])\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c8b6a5",
      "metadata": {
        "id": "50c8b6a5"
      },
      "source": [
        "**Reconstrucción de la película a partir de los modos truncados**\n",
        "\n",
        "Por último, vamos a reconstruir la película, pero únicamente utilizando una cantidad truncada de modos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713cae3f",
      "metadata": {
        "id": "713cae3f"
      },
      "outputs": [],
      "source": [
        "# Número de modos que vamos a utilizar\n",
        "NUM_MODOS = 3\n",
        "\n",
        "# Defino una lista donde acumularemos las imágenes\n",
        "lista_proyeccion = []\n",
        "\n",
        "# Recorro en el tiempo (en nuestro caso son 95 frames)\n",
        "for j in range(Y.shape[1]):\n",
        "\n",
        "    # Defino una imagen vacía donde voy a ir sumando la contribución de cada modo\n",
        "    suma_modos = np.zeros_like(np.reshape(Uhat[:,0],(dims[1],dims[2])))\n",
        "\n",
        "    # Recorro cada modo\n",
        "    for i in range(NUM_MODOS):\n",
        "\n",
        "        # Multiplicamos el modo por su contribución en ese tiempo\n",
        "        mat = np.dot(Uhat[:,i],Vhat[i,j]*Shat[i])\n",
        "\n",
        "        # Le damos la forma adecuada y los sumamos a la imagen\n",
        "        mat = mat.reshape((dims[1],dims[2]))\n",
        "        suma_modos = suma_modos + mat\n",
        "\n",
        "    # Por último transformamos la matriz a valores enteros entre 0 y 255\n",
        "    # (Esto es lo inverso a lo que hicimos al leer los datos)\n",
        "    suma_modos = suma_modos + X_medio\n",
        "    suma_modos = suma_modos * 255\n",
        "    suma_modos = np.clip(suma_modos, 0, 255)\n",
        "    suma_modos = suma_modos.astype('uint8')\n",
        "    lista_proyeccion.append(suma_modos)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e09e3691",
      "metadata": {
        "id": "e09e3691"
      },
      "source": [
        "Guardamos el nuevo gif reconstruido en nuestra carpeta de trabajo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1546ffbc",
      "metadata": {
        "id": "1546ffbc"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "dir_gif = 'proyeccion.gif'\n",
        "imageio.mimsave(dir_gif, lista_proyeccion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b9a9bfd",
      "metadata": {
        "id": "1b9a9bfd"
      },
      "source": [
        "Podemos observar como quedó nuestra pelicula realizada unicamente con N modos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a98a1f4f",
      "metadata": {
        "id": "a98a1f4f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "print('Pelicula generada con ',NUM_MODOS ,'modos')\n",
        "Image(open(dir_gif,'rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38302603",
      "metadata": {
        "id": "38302603"
      },
      "source": [
        "**Ejercicio:** Cambien el número de componentes utilizadas y observen cómo cambia la calidad de la animación reconstruida."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa3350b4",
      "metadata": {
        "id": "fa3350b4"
      },
      "source": [
        "# Espacio Latente con Autoencoder\n",
        "\n",
        "Ahora vamos a trabajar sobre los mismos datos pero usando autoencoders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60e261e",
      "metadata": {
        "id": "f60e261e"
      },
      "outputs": [],
      "source": [
        "seed_value= 42\n",
        "# # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "# # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "# # 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "# # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "# tf.random.set_random_seed(seed_value) # Removed old function\n",
        "tf.random.set_seed(seed_value) # Added new function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a0c2d1",
      "metadata": {
        "id": "48a0c2d1"
      },
      "source": [
        "**Modos con Autoencoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2d1f0d",
      "metadata": {
        "id": "2a2d1f0d"
      },
      "source": [
        "En lugar de utilizar SVD para encontrar modos empíricos de la película, esta vez vamos a utilizar una red neuronal en una arquitectura de **autoencoder** para realizar la reducción dimensional.\n",
        "\n",
        "A diferencia de las redes que vinimos trabajando hasta ahora, un autoencoder no busca asociar a cada instancia a un valor categórico o numérico, no hay un *target*. Este tipo de algoritmos se denominan de Aprendizaje no supervisado. En este caso, la red se entrena para comprimir la representación de las instancia de entrada de la manera más eficiente posible: intenta reproducir en la salida los mismos valores que hay a su entrada. Es por esto que el número de neuronas en la última capa debe ser igual al número de neuronas que hay a la entrada `numero_pixeles`.\n",
        "\n",
        "La arquitectura de este tipo de redes suele ser simétrica llamando encoder a la primera parte (todo lo que está antes del cuello de botella, el `latent space`) y decoder a la segunda parte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbaf22d0",
      "metadata": {
        "id": "fbaf22d0"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation # Updated import\n",
        "\n",
        "numero_pixeles = Y.shape[1]\n",
        "NUM_MODOS = 3\n",
        "\n",
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Encoder: comprimimos la representación achicando el número de neuronas\n",
        "model.add(Dense(64, input_shape=(numero_pixeles,)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(NUM_MODOS, activation='linear')) # Latent Space: Punto máximo de la compresión\n",
        "\n",
        "# Decoder: llevamos nuevamente al mismo tamaño que la entrada\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(units=numero_pixeles, activation='linear'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe8c27c9",
      "metadata": {
        "id": "fe8c27c9"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mse', metrics=['mean_absolute_error'], optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67ba31f5",
      "metadata": {
        "id": "67ba31f5"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0c73a1",
      "metadata": {
        "id": "bc0c73a1"
      },
      "source": [
        "Como dijimos, nuestro objetivo será reproducir la entrada, por lo tanto a la hora de entrenar el modelo debemos pasarle como salida el mismo dataset que a la entrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "781884b1",
      "metadata": {
        "id": "781884b1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# training the model and saving metrics in history\n",
        "history = model.fit(Y, Y,\n",
        "          batch_size=16,\n",
        "          epochs=500,\n",
        "          validation_split=0.25,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a999d28",
      "metadata": {
        "id": "9a999d28"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (10,4))\n",
        "plt.subplot(1,1,1)\n",
        "plt.plot(history.history['mean_absolute_error'])\n",
        "plt.plot(history.history['val_mean_absolute_error'])\n",
        "plt.title('model mean_absolute_error')\n",
        "plt.ylabel('mean_absolute_error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='lower right')\n",
        "plt.ylim(0,0.04)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f259b5",
      "metadata": {
        "id": "e2f259b5"
      },
      "outputs": [],
      "source": [
        "# # save model\n",
        "# model.save('model_autoencoder.h5')\n",
        "# # load model\n",
        "# # from keras.models import load_model\n",
        "# model = keras.models.load_model('model_autoencoder.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01faa9bd",
      "metadata": {
        "id": "01faa9bd"
      },
      "source": [
        "Para poder acceder al `Latent Space` (la capa con la menor cantidad de neuronas), vamos a definir una función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e6f5df",
      "metadata": {
        "id": "e9e6f5df"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "get_latent_layer_output = K.function([model.layers[0].input],[model.layers[3].output])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbfd5f5",
      "metadata": {
        "id": "cdbfd5f5"
      },
      "source": [
        "Ahora sí, veamos cómo queda representado el dataset en el `Latent Space`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5ef4c0",
      "metadata": {
        "id": "7d5ef4c0"
      },
      "outputs": [],
      "source": [
        "layer_output = get_latent_layer_output ([Y])[:][0]\n",
        "print(layer_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bf24d2",
      "metadata": {
        "id": "e3bf24d2"
      },
      "source": [
        "Noten que ahora tenemos una cantidad `NUM_MODOS` por cada instancia (por cada frame)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fd5821",
      "metadata": {
        "id": "80fd5821"
      },
      "source": [
        "**Evolución Temporal de los modos**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2867297",
      "metadata": {
        "id": "f2867297"
      },
      "source": [
        "Exploremos cómo evolucionan temporalmente los primeros modos empíricos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36b06c05",
      "metadata": {
        "id": "36b06c05"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,1,1)\n",
        "plt.plot(layer_output[:,0],'.-',label='Modo 1')\n",
        "plt.plot(layer_output[:,1],'.-',label='Modo 2')\n",
        "plt.plot(layer_output[:,2],'.-',label='Modo 3')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cc00ff",
      "metadata": {
        "id": "76cc00ff"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.plot(layer_output[:,0],layer_output[:,1],layer_output[:,2])\n",
        "ax.set_xlabel('Mode 1')\n",
        "ax.set_ylabel('Mode 2')\n",
        "ax.set_zlabel('Mode 3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eae35c5",
      "metadata": {
        "id": "6eae35c5"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5,5))\n",
        "plt.plot(layer_output[:,1],layer_output[:,2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "babd5d32",
      "metadata": {
        "id": "babd5d32"
      },
      "source": [
        "**Reconstrucción de la película**\n",
        "\n",
        "Por último, vamos a reconstruir la película, pero utilizando la salida de la red. Esto sería equivalente a utilizar los modos encontrados, ya que los valores que vemos a la salida de la red están determinados por los valores en las neuronas del `Latent Space`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8384e95d",
      "metadata": {
        "id": "8384e95d"
      },
      "outputs": [],
      "source": [
        "# Defino una lista donde acumularemos las imágenes\n",
        "lista_proyeccion = []\n",
        "\n",
        "# Recorro en el tiempo (en nuestro caso son 95 frames)\n",
        "for j in range(Y.shape[0]):\n",
        "\n",
        "    suma_modos = model.predict(Y[j].reshape((1,numero_pixeles)))\n",
        "    suma_modos = suma_modos.reshape((dims[1],dims[2]))\n",
        "    # Por último Transformamos la matriz a valores enteros entre 0 y 255\n",
        "    # (Esto es lo inverso a lo que hicimos al leer los datos)\n",
        "    suma_modos = suma_modos + X_medio\n",
        "    suma_modos = suma_modos * 255\n",
        "    suma_modos = np.clip(suma_modos, 0, 255)\n",
        "    suma_modos = suma_modos.astype('uint8')\n",
        "    lista_proyeccion.append(suma_modos)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb494904",
      "metadata": {
        "id": "fb494904"
      },
      "source": [
        "Guardamos el nuevo gif reconstruido en nuestra carpeta de trabajo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab002672",
      "metadata": {
        "id": "ab002672"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "dir_gif = 'proyeccion_autoencoder.gif'\n",
        "imageio.mimsave(dir_gif, lista_proyeccion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1024416e",
      "metadata": {
        "id": "1024416e"
      },
      "source": [
        "Podemos observar como quedó nuestra película realizada únicamente con N modos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c393c10e",
      "metadata": {
        "id": "c393c10e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "print('Pelicula generada con ',NUM_MODOS ,'modos')\n",
        "Image(open(dir_gif,'rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003386f1",
      "metadata": {
        "id": "003386f1"
      },
      "source": [
        "**Ejercicio:** Compare el desempeño de la descomposición modal realizada con el autoencoder contra el mecanismo de SVD para distintos valores de N (número de modos). Para esto, utilice como métrica el Mean square error (diferencia entre imagen original y reconstruida)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b8ff4c",
      "metadata": {
        "id": "41b8ff4c"
      },
      "outputs": [],
      "source": [
        "Y_proy = np.transpose(np.reshape(np.array(lista_proyeccion),(dims[0],dims[1]*dims[2]))).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b802397",
      "metadata": {
        "id": "1b802397"
      },
      "outputs": [],
      "source": [
        "# from keras import metrics\n",
        "# metrics.mean_squared_error(Y, Y_proy)\n",
        "# np.mean(np.square(Y - Y_proy), axis=-1)\n",
        "np.mean((Y_proy - Y)**2, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6e5badb",
      "metadata": {
        "id": "e6e5badb"
      },
      "source": [
        "## Extra: Interpretación de los modos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da7522ad",
      "metadata": {
        "id": "da7522ad"
      },
      "source": [
        "En este esquema de trabajo es dificil interpretar a qué corresponde cada modo. De hecho, al tratarse de modos no lineales, mirando la activación del modo 1 y del modo 2 por separado, no es posible inferir cómo será la interacción de ambos modos activos al mismo tiempo. Más aún, una activación de un único modo con intensidad 2, no necesariamente genera una imagen que sea equivalente 2 veces la imgen generada por eso modo con intensidad 1.\n",
        "\n",
        "Sin embargo, podemos hacer un intento y graficar cómo se verían las imágenes para cada modo activado por separado.\n",
        "\n",
        "Para esto será necesario definir el modelo de otra forma, ya que luego vamos a querer definir \"a mano\" el valor de activación en la capa correspondiente al `latent space`.\n",
        "\n",
        "**Importante:** No es necesario que comprendan el código a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351b1fa1",
      "metadata": {
        "id": "351b1fa1"
      },
      "outputs": [],
      "source": [
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "numero_pixeles = Y.shape[1]\n",
        "NUM_MODOS = 3\n",
        "\n",
        "\n",
        "# Definimos las capas que del encoder y cómo se conectan\n",
        "input_img= Input(shape=(numero_pixeles,))\n",
        "encoded = Dense(units=64, activation='relu')(input_img)\n",
        "encoded = Dense(units=32, activation='relu')(encoded)\n",
        "encoded = Dense(units=16, activation='relu')(encoded)\n",
        "\n",
        "# Usamos neuronas que pueden tomar valores negativos para la hidden layer.\n",
        "encoded = Dense(units=NUM_MODOS, activation='linear')(encoded)\n",
        "\n",
        "# Definimos las capas del decoder\n",
        "decoder_1 = Dense(units=16, activation='relu')\n",
        "decoder_2 = Dense(units=32, activation='relu')\n",
        "decoder_3 = Dense(units=64, activation='relu')\n",
        "decoder_4 = Dense(units=numero_pixeles, activation='linear')\n",
        "\n",
        "# Definimos cómo se conectan las capas del decoder en el autoencoder\n",
        "decoder_1_output = decoder_1(encoded)\n",
        "decoder_2_output = decoder_2(decoder_1_output)\n",
        "decoder_3_output = decoder_3(decoder_2_output)\n",
        "decoder_4_output = decoder_4(decoder_3_output)\n",
        "\n",
        "# Preparamos unas capas para el modelo que nos lleve del\n",
        "# valor de las 3 neuronas del latent space a una imagen (el decoder)\n",
        "input_modos = Input(shape=(NUM_MODOS,))\n",
        "modos_1_act = decoder_1(input_modos)\n",
        "modos_2_act = decoder_2(modos_1_act)\n",
        "modos_3_act = decoder_3(modos_2_act)\n",
        "modos_4_act = decoder_4(modos_3_act)\n",
        "\n",
        "\n",
        "# Definimos el modelo autoencoder y el encoder\n",
        "autoencoder = Model(input_img, decoder_4_output)\n",
        "encoder = Model(input_img, encoded)\n",
        "\n",
        "# Definimos un modelo para inspeccionar los modos\n",
        "modos_activacion = Model(input_modos, modos_4_act)\n",
        "\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d583d0f",
      "metadata": {
        "id": "0d583d0f"
      },
      "source": [
        "Compilamos y entrenamos el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738457db",
      "metadata": {
        "id": "738457db",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(loss='mse', metrics=['mean_absolute_error'], optimizer='adam')\n",
        "# training the model and saving metrics in history\n",
        "history = autoencoder.fit(Y, Y,\n",
        "          batch_size=16,\n",
        "          epochs=500,\n",
        "          validation_split=0.25,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3351f2",
      "metadata": {
        "id": "7d3351f2"
      },
      "source": [
        "Graficamos la activación de los modos para las imágenes del dataset en función del tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c185cd",
      "metadata": {
        "id": "44c185cd"
      },
      "outputs": [],
      "source": [
        "# Usamos un modelo que nos lleva de los datos a la capa intermedia (latent space)\n",
        "layer_output = encoder.predict(Y)\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,1,1)\n",
        "plt.plot(layer_output[:,0],'.-',label='Modo 1')\n",
        "plt.plot(layer_output[:,1],'.-',label='Modo 2')\n",
        "plt.plot(layer_output[:,2],'.-',label='Modo 3')\n",
        "plt.legend()Interpretación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d5a0f2",
      "metadata": {
        "id": "15d5a0f2"
      },
      "outputs": [],
      "source": [
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9bd7ac",
      "metadata": {
        "id": "1d9bd7ac"
      },
      "source": [
        "Observe el valor de activación de cada una de las 3 neuronas (eje y) en la serie temporal graficada en la celda anterior. Tome estos valores como una referencia de que puede considerar como valor de activación para cada uno de los modos.\n",
        "\n",
        "Por último, vamos a imponerle valores a la capa intermedia del modelo y ver cuál es la imagen a la salida.\n",
        "\n",
        "Recuerden algo importante: en este esquema no hay una jerarquía de importancia entre los modos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f45af3",
      "metadata": {
        "id": "24f45af3"
      },
      "outputs": [],
      "source": [
        "# Elegimos el valor de las neuronas en la capa intermedia\n",
        "# Y guardamos la salida del autoencoder para cada uno de los modos\n",
        "modo_1 = modos_activacion.predict(np.asarray([0,0,10]).reshape(1,3))\n",
        "modo_2 = modos_activacion.predict(np.asarray([0,10,0]).reshape(1,3))\n",
        "modo_3 = modos_activacion.predict(np.asarray([10,0,0]).reshape(1,3))\n",
        "\n",
        "\n",
        "# Graficamos\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(np.reshape(modo_1,(dims[1],dims[2])),cmap='gray',vmin=np.min(modo_1),vmax=np.max(modo_1))\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(np.reshape(modo_2,(dims[1],dims[2])),cmap='gray',vmin=np.min(modo_2),vmax=np.max(modo_2))\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(np.reshape(modo_3,(dims[1],dims[2])),cmap='gray',vmin=np.min(modo_3),vmax=np.max(modo_3))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a884e6dc",
      "metadata": {
        "id": "a884e6dc"
      },
      "source": [
        "Como verán, es muy dificil interpretar los modos. Esto evidencia un trade-off bastante universal: una reducción de dimensionalidad más efectiva (no lineal) conlleva una pérdida de intepretabilidad de los modos encontrados."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1\n",
        "\n",
        "Ahora vamos a aplicar estas estrategias para extraer automáticamente la dinámica desde un video a partir del oscilador amortiguado que vimos en las clases de *ajuste de datos* y de *reconstrucción de ODEs*. De esta manera vamos a poder reconstruir el espacio de fases a partir de encontrar trayectorias en el espacio latente del autoencoder, cuya dinámica es topológicamente equivalente al espacio de fases original."
      ],
      "metadata": {
        "id": "4UsMZgMFddvs"
      },
      "id": "4UsMZgMFddvs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a25cfd8",
      "metadata": {
        "id": "8a25cfd8"
      },
      "outputs": [],
      "source": [
        "# # # COMPLETAR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2\n",
        "\n",
        "Como vieron, esto es una herramienta muy poderosa para encontrar oscilaciones a partir de videos muy generales. Busquen de probar con algún video que pueda serles de interés. Por ejemplo, en el video de este link \"https://www.youtube.com/watch?v=ASqqp0a8LqI\", al final, a partir del minuto 3:50, tienen una oscilación muy peculiar. Pero busquen posibles videos que puedan aprovechar con estas herramientas."
      ],
      "metadata": {
        "id": "nGqddeDLeIR6"
      },
      "id": "nGqddeDLeIR6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccb222a",
      "metadata": {
        "id": "1ccb222a"
      },
      "outputs": [],
      "source": [
        "# # # COMPLETAR"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}